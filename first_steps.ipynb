{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note: \n",
    "\n",
    "* After finishing this project the API_KEY shown here won't be working anymore. Having said that, if you try to use this code in future, may you wish to generate a valid key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to set your key as an enviroment variable that you can afterward retrieve it. \n",
    "\n",
    "* To create an enviroment variable: \n",
    "` os.environ['name of the env. variable'] = 'key' `\n",
    "\n",
    "* To retrieve the variable\n",
    "`os.getenv('name of the variable')`\n",
    "\n",
    "For the example below, I am going to create a variable called 'OPENAI_API_KEY' and inform the 'key' I created for testing. \n",
    "!Notice, it is important that once you've created this key and settled your environment variable, you delete the key from your code. So it won't be easily seen and available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']='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'\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alternatives solution to avoid showing the API key in your code. \n",
    "\n",
    "* Solution 1: \n",
    "> Using the library 'getpass' you can make the code ask you about the key.\n",
    "\n",
    "```\n",
    "import getpass\n",
    "key = getpass.getpass('Paste your API key:')\n",
    "openai.apikey = key\n",
    "```\n",
    "\n",
    "* Solution 2: \n",
    "> Save your api key into a file and call the correct opeanai module to read it from there\n",
    "```\n",
    "openai.api_key = open('key.txt').read().strip('\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI model is a software program that uses specific ML and DL algo and has bein trained on a set of data to perform specific tasks. \n",
    "OPENAI offers a family of models with different capabilities. Each model can be customized by 'fine-tuning' it. It means you can adjust things to have your flavour. \n",
    "\n",
    "!Be sure to have a look at the set of models provided by OPEN_AI and its functionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The prompt \n",
    "\n",
    "THe prompt is a piece of text of instructions that is given to an AI model to guide it in generating a specific output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricing\n",
    "\n",
    "Currently the usage of chatGPT isn't free of charge. Information about how it costs may be found at `openai.com/pricing`\n",
    "\n",
    "Here is an example of the price (dating 26th Jan 2024): \n",
    "> gpt-3.5-turbo-1106\t$0.0010 / 1K tokens\t$0.0020 / 1K tokens\n",
    "\n",
    "* Fining-tune models \n",
    "> gpt-3.5-turbo\t$0.0080 / 1K tokens\t$0.0030 / 1K tokens\t$0.0060 / 1K tokens\n",
    "\n",
    "1000 tokens = 750 words (english based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens \n",
    "\n",
    "Each AI model has its own limit with regarding the number of tokens used to request/retrieve information. \n",
    "For example, the model we'll be using here is currently tighten to 4096 tokens. \n",
    "\n",
    "> `gpt-3.5-turbo 4,096 tokens`\n",
    "\n",
    "* What are tokens? \n",
    "\n",
    "> Tokens are peices of words, before the API processes the prompt, the input is browken down into tokens. \n",
    "\n",
    "> Tokens can be words or just chuncks of characters.\n",
    "\n",
    "> 1 token is approximately 4 characters or 0.75 words for English text. \n",
    "\n",
    "\n",
    "It is important to be aware of how tokens are computized. There are many ways you can see how many tokens your text provides to the API. \n",
    "\n",
    "1. Let's assume you have a response. Just type the command `print(response.usage)`\n",
    "2. You can use the OpenAI python library called `tiktoken`\n",
    "3. Utilize the `tokenizer` also from OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat.completions\n",
    "\n",
    "Let's discuss some of the arguments of the function `client.chat.completions.create()`\n",
    "\n",
    "* model: indicates the model type you want to use \n",
    "\n",
    "* messages: it is a list of dictionaries. Each dictionary has two properties, role and content.\n",
    "- The roles are 'system', 'user', and 'assistant'\n",
    ">  message = [{system}, {user}, {assistant}]\n",
    ">  message = [{role:content}, {role:content}, {role:content}]\n",
    "- System: It helps set the behavior of the assistant, you can extract the model to play a specific role \n",
    "- User: It is the prompt for what you ask the assistante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Indulge in frozen bliss!\"\n",
      "CompletionUsage(completion_tokens=16, prompt_tokens=32, total_tokens=48)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# Get a list of all available models\n",
    "# A paid account has a more extensive list of available models\n",
    "def print_available_models():\n",
    "    cm = client.models.list()\n",
    "    for models in cm: \n",
    "        print(models)\n",
    "\n",
    "# Notice that it explains and ends the explanation with a question\n",
    "system_role_content1 = 'You explain concepts in depth using simple terms, and you give examples to help people learn. At the end of each explanation you ask a question to check for understanding'\n",
    "system_role_content2 = 'You are a concise assistant. You reply briefly with no elaboration'\n",
    "\n",
    "# Setting the personality of the model and giving specific instructions on how to response some types of response and also injecting instructions into the model's reponse\n",
    "system_role_content3 = 'You reply in the style of Yoda character from Star Wars'\n",
    "\n",
    "# To create a gpt request\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo', \n",
    "    messages=[\n",
    "        {'role': 'system', 'content': system_role_content2},\n",
    "        {'role': 'system', 'content': 'Write a short article about coffee.'}, \n",
    "    ],\n",
    "    # it controls how creative and random the model is (openai models are non deterministic, i.e., same input gives diff. results )\n",
    "    # large temperature -> more deterministic and more predictable results. (can be set between 0-2, default = 1.0)\n",
    "    # low temperature   -> a few answers or even a single one to generate ideas or a complete story. \n",
    "    # large temperature -> diverse but it has a probability to hallucination.\n",
    "    temperature=1,\n",
    "    # more updated versions allows a parameter for predictable outputs\n",
    "    # seed = integer -> it retrieves the system_fingertype of where the answer was generated. If the same system_fingerprint is used across different API request calls, it is likely you gonna receive different answers. \n",
    "    # by setting 'seed' you increase the deterministic parameter so you get the same answer acrros different api calls.\n",
    "    # seed='1234'\n",
    "    \n",
    "    # Nucleous sampling. It controls how much the model focus on the most likely completions. \n",
    "    # For example: 0.2 -> only the tokens comprizing on the top 20% probability mass (most commom tokens) are taken into consideration. \n",
    "    # For example: 1.0 -> means use all tokens on the vocabulary \n",
    "    # You should avoid use top_p and temperature simultaneously. \n",
    "    # top_p=0.2,\n",
    "    \n",
    "    # be careful with this one because it can cut off the final answer for the user. \n",
    "    # max_tokens=\n",
    "    \n",
    "    # Set the total number of different answers\n",
    "    # n=2\n",
    "    \n",
    "    # Used to make the model stop . For example: one line answer means stop when making a breaking line '\\n'\n",
    "    # it can also be used a list stop=[';', '.', \"\\n\"]. ps: some symbols are very useful if you are generating a SELECT SQL query output\n",
    "    # stop='\\n'\n",
    "    \n",
    "    # frequency_penalty=0 # is by defaul 0. Can be set between -2 and +2. Lower value make the model repeat itself more often. It controls the quality of the answers.\n",
    "    # presence_penalty=0 # is by defaul 0 and [-2,+2] that encorages the model to use a diverse range of words that generates; dont just focus into the most commom words, but use the other words of the dictionaries too.\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Indulge in Frozen Bliss.\"\n",
      "CompletionUsage(completion_tokens=16, prompt_tokens=32, total_tokens=48)\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[1].message.content)\n",
    "print(response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model='gpt-3.5-turbo', \n",
    "messages=[\n",
    "    {'role': 'system', 'content': system_role_content2},\n",
    "    {'role': 'system', 'content': 'Write one sentecne review of 1984 by George Orwell'}, \n",
    "],\n",
    "```\n",
    "\n",
    "Answer with h_top=1:\n",
    " \n",
    "\"1984\" by George Orwell is a chilling and thought-provoking dystopian novel that explores the dangers of totalitarianism and the erosion of individual freedom.\n",
    "> CompletionUsage(completion_tokens=32, prompt_tokens=37, total_tokens=69)\n",
    "\n",
    "Answer with h_top=0.2:\n",
    "\n",
    "\"1984\" by George Orwell is a chilling and thought-provoking dystopian novel that remains relevant today.\n",
    "> CompletionUsage(completion_tokens=23, prompt_tokens=37, total_tokens=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How GPT Models work? \n",
    "### Highlights of the important aspects (A very high overview perspective)\n",
    "\n",
    "Open AI's GPT family of models are all LLMs (Large Language Models)\n",
    "\n",
    "An LLM is a type of AI that can generate and understand human language. \n",
    "\n",
    "A large ammount of text is used to traning and allow the machine to learn our language. \n",
    "\n",
    "LLMs key components: \n",
    "1. `Self-Attention Mechanism` - allos the model to fpocus on the most relevant part of the input text when generating or understanding language \n",
    "\n",
    "2. `Reinforcement Learning from Human feedback (RLHF)` - involves traning the model to generate resposnes that are informative , factual, engaging, and relevant to the conversations context. \n",
    "\n",
    "`GPT = Generative Pre-trained transformer `\n",
    "\n",
    "* `Generative` = the model is designed to generate new output based on the input it is given\n",
    "\n",
    "* `Pre-trained` = the model was trained on a large corpus of data before being fine-tuned on a specific task \n",
    "\n",
    "* `Transformer` = which is a type of neural network that utilizes self-attention mechanicsm to evaluate the significance of various input tokens when generating output. \n",
    "\n",
    "LLMs - huge quantities of text data and infer relationships between words within the text; enhance their capabilites as the size of their input datasets and parameter space increase\n",
    "\n",
    "## LLMs Issues and Limitations \n",
    "\n",
    "* Capability - accuracy, fluency, creativity, adaptability, and robustness\n",
    "* Alignment  - to what extent the model's goals and behavior align with human values and expectations\n",
    "\n",
    "LLMs are prone to misalignment. This misalignment comes from probability. \n",
    "* Model hallucinations, \n",
    "* lack of interpratbility\n",
    "* generating biased or toxic output. \n",
    "\n",
    "GPT3, 3.5-Turbo, 4, are generative modes with the following core techniques: \n",
    "* Next-token prediction\n",
    "This defines the words to be said in a sequence in a human like format. Given a sequence of words as input, which word would be the most probable one? (that makes it sound correct, and human)\n",
    "\n",
    "* Masked-Language modeling\n",
    "It is a variant of the next-token prediction. \n",
    "\n",
    "> `Interesting to read` \n",
    "Paper: Training language models to follow intructions with human feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "Prompt engineering is the art of crafting intructions for LLMs to get desired responses. \n",
    "\n",
    "`Tasks -> Provide Contexts -> Provides desirable outcomes`\n",
    "\n",
    "## Best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os \n",
    "os.environ['OPENAI_API_KEY']='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'\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_chat_completion(\n",
    "    user_prompt, \n",
    "    system_role='You are a helpful assistant',\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=1\n",
    "    ):\n",
    "    \n",
    "    messages=[\n",
    "        {'role': 'system', 'content': system_role},\n",
    "        {'role': 'user',   'content': user_prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages, \n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average distance from Earth to Mars is approximately 225 million kilometers (140 million miles) when the two planets are at their closest approach. However, this distance can vary greatly depending on the positions in their orbits.\n"
     ]
    }
   ],
   "source": [
    "response=get_chat_completion('what is the distance to Mars in km?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
